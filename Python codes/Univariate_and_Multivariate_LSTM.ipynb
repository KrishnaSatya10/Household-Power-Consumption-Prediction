{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Univariate and Multivariate LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iMhDdZj_gDx"
      },
      "source": [
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime as dt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from fbprophet import Prophet\n",
        "from google.colab import files\n",
        "from sklearn.metrics import mean_absolute_error as mae\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ-EvCncDE7D"
      },
      "source": [
        "# Reading the data and basic sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg71Vx5NDGyl"
      },
      "source": [
        "#Reading the data from text file\n",
        "meter_data = pd.read_table('/household_power_consumption.txt', sep= ';',parse_dates=[['Date','Time']],infer_datetime_format=True,low_memory=False,na_values=['nan','?'])\n",
        "meter_data.set_index('Date_Time', inplace = True)\n",
        "\n",
        "#Reading the weather data from csv (web-scraped file)\n",
        "weather_data= pd.read_csv('/Weather.csv')\n",
        "weather_data['Date']= pd.to_datetime(weather_data['Date'])\n",
        "weather_data.set_index('Date', inplace = True)\n",
        "\n",
        "meter_data.interpolate(method='linear',limit_direction='forward', axis=0, inplace=True)\n",
        "\n",
        "weather_data.drop('Precipitation', axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV5l8razEAA2"
      },
      "source": [
        "# Resampling at daily,weekly and monthly levels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDV4iFrOzj6D"
      },
      "source": [
        "## For Univariate case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "3GUIScV9znTH",
        "outputId": "6ff0f588-4115-40d5-eb8c-242c2f346d30"
      },
      "source": [
        "#Resampling meter data at daily, weekly and monthly levels\n",
        "meter_data_daily = pd.DataFrame(meter_data['Global_active_power'].resample('D').sum())\n",
        "meter_data_weekly = pd.DataFrame(meter_data['Global_active_power'].resample('W').sum())\n",
        "meter_data_monthly = pd.DataFrame(meter_data['Global_active_power'].resample('M').sum())\n",
        "\n",
        "# Creating a column with the power consumption values of the previous instance \n",
        "\n",
        "# for days\n",
        "meter_data_daily_prev = pd.DataFrame(meter_data['Global_active_power'].resample('D').sum())\n",
        "meter_data_daily_prev.columns = ['Previous day Power']\n",
        "meter_data_daily_prev = meter_data_daily_prev.shift(1)\n",
        "\n",
        "# for weeks\n",
        "meter_data_weekly_prev = pd.DataFrame(meter_data['Global_active_power'].resample('W').sum())\n",
        "meter_data_weekly_prev.columns = ['Previous week Power']\n",
        "meter_data_weekly_prev = meter_data_weekly_prev.shift(1)\n",
        "\n",
        "# for months\n",
        "meter_data_monthly_prev = pd.DataFrame(meter_data['Global_active_power'].resample('M').sum())\n",
        "meter_data_monthly_prev.columns = ['Previous month Power']\n",
        "meter_data_monthly_prev = meter_data_monthly_prev.shift(1)\n",
        "\n",
        "\n",
        "#Merge the response column from meter data with weather data for daily, weekly and monthly data\n",
        "\n",
        "data_response_daily = meter_data_daily_prev.merge(meter_data_daily, left_index = True,right_index = True)\n",
        "\n",
        "data_response_weekly = meter_data_weekly_prev.merge(meter_data_weekly, left_index = True,right_index = True)\n",
        "\n",
        "data_response_monthly = meter_data_monthly_prev.merge(meter_data_monthly, left_index = True,right_index = True)\n",
        "\n",
        "\n",
        "#Sanity check\n",
        "data_response_daily \n",
        "#data_response_weekly\n",
        "# data_response_monthly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Previous day Power</th>\n",
              "      <th>Global_active_power</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date_Time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2006-12-16</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1209.176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-12-17</th>\n",
              "      <td>1209.176</td>\n",
              "      <td>3390.460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-12-18</th>\n",
              "      <td>3390.460</td>\n",
              "      <td>2203.826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-12-19</th>\n",
              "      <td>2203.826</td>\n",
              "      <td>1666.194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-12-20</th>\n",
              "      <td>1666.194</td>\n",
              "      <td>2225.748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-11-22</th>\n",
              "      <td>900.910</td>\n",
              "      <td>2041.536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-11-23</th>\n",
              "      <td>2041.536</td>\n",
              "      <td>1577.536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-11-24</th>\n",
              "      <td>1577.536</td>\n",
              "      <td>1796.248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-11-25</th>\n",
              "      <td>1796.248</td>\n",
              "      <td>1431.164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-11-26</th>\n",
              "      <td>1431.164</td>\n",
              "      <td>1488.104</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1442 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Previous day Power  Global_active_power\n",
              "Date_Time                                          \n",
              "2006-12-16                 NaN             1209.176\n",
              "2006-12-17            1209.176             3390.460\n",
              "2006-12-18            3390.460             2203.826\n",
              "2006-12-19            2203.826             1666.194\n",
              "2006-12-20            1666.194             2225.748\n",
              "...                        ...                  ...\n",
              "2010-11-22             900.910             2041.536\n",
              "2010-11-23            2041.536             1577.536\n",
              "2010-11-24            1577.536             1796.248\n",
              "2010-11-25            1796.248             1431.164\n",
              "2010-11-26            1431.164             1488.104\n",
              "\n",
              "[1442 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fre78HQzbae"
      },
      "source": [
        "## For Multivariate case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "nX4OJ_haEdOb",
        "outputId": "3eaf99f0-fcdb-4cc8-d43b-82d9ec4d922f"
      },
      "source": [
        "#Resampling meter data at daily, weekly and monthly levels\n",
        "meter_data_daily = pd.DataFrame(meter_data['Global_active_power'].resample('D').sum())\n",
        "meter_data_weekly = pd.DataFrame(meter_data['Global_active_power'].resample('W').sum())\n",
        "meter_data_monthly = pd.DataFrame(meter_data['Global_active_power'].resample('M').sum())\n",
        "\n",
        "# Creating a column with the power consumption values of the previous instance \n",
        "\n",
        "# for days\n",
        "meter_data_daily_prev = pd.DataFrame(meter_data['Global_active_power'].resample('D').sum())\n",
        "meter_data_daily_prev.columns = ['Previous day Power']\n",
        "meter_data_daily_prev = meter_data_daily_prev.shift(1)\n",
        "\n",
        "# for weeks\n",
        "meter_data_weekly_prev = pd.DataFrame(meter_data['Global_active_power'].resample('W').sum())\n",
        "meter_data_weekly_prev.columns = ['Previous week Power']\n",
        "meter_data_weekly_prev = meter_data_weekly_prev.shift(1)\n",
        "\n",
        "# for months\n",
        "meter_data_monthly_prev = pd.DataFrame(meter_data['Global_active_power'].resample('M').sum())\n",
        "meter_data_monthly_prev.columns = ['Previous month Power']\n",
        "meter_data_monthly_prev = meter_data_monthly_prev.shift(1)\n",
        "\n",
        "\n",
        "#Resampling meter data at daily, weekly and monthly levels\n",
        "weather_data_daily  = weather_data.copy()\n",
        "weather_data_weekly  = weather_data.resample('W').mean()\n",
        "weather_data_monthly  = weather_data.resample('M').mean()\n",
        "\n",
        "\n",
        "\n",
        "#Merge the response column from meter data with weather data for daily, weekly and monthly data\n",
        "\n",
        "weather_response_daily = weather_data_daily.shift(1).merge(meter_data_daily_prev, left_index = True,right_index = True)\n",
        "data_response_daily = weather_response_daily.merge(meter_data_daily, left_index = True,right_index = True)\n",
        "\n",
        "weather_response_weekly = weather_data_weekly.merge(meter_data_weekly_prev, left_index = True,right_index = True)\n",
        "data_response_weekly = weather_response_weekly.merge(meter_data_weekly, left_index = True,right_index = True)\n",
        "\n",
        "weather_response_monthly = weather_data_monthly.merge(meter_data_monthly_prev, left_index = True,right_index = True)\n",
        "data_response_monthly = weather_response_monthly.merge(meter_data_monthly, left_index = True,right_index = True)\n",
        "\n",
        "data_response_daily.drop(['Dew Point','Pressure'],axis = 1, inplace = True)\n",
        "data_response_weekly.drop(['Dew Point','Pressure'],axis = 1, inplace = True)\n",
        "data_response_monthly.drop(['Dew Point','Pressure'],axis = 1, inplace = True)\n",
        "\n",
        "#Sanity check\n",
        "data_response_daily \n",
        "# weather_response_weekly\n",
        "# weather_response_monthly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Temperature</th>\n",
              "      <th>Humidity</th>\n",
              "      <th>Wind Speed</th>\n",
              "      <th>Previous day Power</th>\n",
              "      <th>Global_active_power</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2006-12-16</th>\n",
              "      <td>28.9</td>\n",
              "      <td>88.3</td>\n",
              "      <td>5.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1209.176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-12-17</th>\n",
              "      <td>42.2</td>\n",
              "      <td>90.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>1209.176</td>\n",
              "      <td>3390.460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-12-18</th>\n",
              "      <td>25.1</td>\n",
              "      <td>93.6</td>\n",
              "      <td>3.4</td>\n",
              "      <td>3390.460</td>\n",
              "      <td>2203.826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-12-19</th>\n",
              "      <td>38.1</td>\n",
              "      <td>91.6</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2203.826</td>\n",
              "      <td>1666.194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-12-20</th>\n",
              "      <td>36.0</td>\n",
              "      <td>82.3</td>\n",
              "      <td>11.5</td>\n",
              "      <td>1666.194</td>\n",
              "      <td>2225.748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-11-22</th>\n",
              "      <td>43.0</td>\n",
              "      <td>94.6</td>\n",
              "      <td>8.3</td>\n",
              "      <td>900.910</td>\n",
              "      <td>2041.536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-11-23</th>\n",
              "      <td>39.8</td>\n",
              "      <td>87.1</td>\n",
              "      <td>7.2</td>\n",
              "      <td>2041.536</td>\n",
              "      <td>1577.536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-11-24</th>\n",
              "      <td>38.6</td>\n",
              "      <td>89.5</td>\n",
              "      <td>5.3</td>\n",
              "      <td>1577.536</td>\n",
              "      <td>1796.248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-11-25</th>\n",
              "      <td>40.3</td>\n",
              "      <td>89.2</td>\n",
              "      <td>6.8</td>\n",
              "      <td>1796.248</td>\n",
              "      <td>1431.164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-11-26</th>\n",
              "      <td>36.4</td>\n",
              "      <td>92.6</td>\n",
              "      <td>5.7</td>\n",
              "      <td>1431.164</td>\n",
              "      <td>1488.104</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1442 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Temperature  Humidity  ...  Previous day Power  Global_active_power\n",
              "2006-12-16         28.9      88.3  ...                 NaN             1209.176\n",
              "2006-12-17         42.2      90.0  ...            1209.176             3390.460\n",
              "2006-12-18         25.1      93.6  ...            3390.460             2203.826\n",
              "2006-12-19         38.1      91.6  ...            2203.826             1666.194\n",
              "2006-12-20         36.0      82.3  ...            1666.194             2225.748\n",
              "...                 ...       ...  ...                 ...                  ...\n",
              "2010-11-22         43.0      94.6  ...             900.910             2041.536\n",
              "2010-11-23         39.8      87.1  ...            2041.536             1577.536\n",
              "2010-11-24         38.6      89.5  ...            1577.536             1796.248\n",
              "2010-11-25         40.3      89.2  ...            1796.248             1431.164\n",
              "2010-11-26         36.4      92.6  ...            1431.164             1488.104\n",
              "\n",
              "[1442 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNCKRdl6r7nt"
      },
      "source": [
        "# Model Training and prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLZZjniqsA_r"
      },
      "source": [
        "## For days"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASZfdtmHr6pr"
      },
      "source": [
        "dataset = weather_response_daily\n",
        "train,test = dataset.values[:-331],dataset.values[-331:]\n",
        "#dataset.iloc[1:-331,:-1]\n",
        "#dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWoMcU9kAD6w"
      },
      "source": [
        "### For 1 layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06nxiVz_GVe9"
      },
      "source": [
        "def Multivariate_LSTM(X):\n",
        "  #Splitting the data into train and test sets. Since we are dealing with time-series data we cannot split it into randomized split. It must be a sequential split.\n",
        "  #Here we are using first 75% data for training and 25% data for testing\n",
        "  #X_train = X.iloc[:int(X.shape[0]*0.80),:]\n",
        "  #X_test = X.iloc[int(X.shape[0]*0.80):,:]\n",
        "  #y_train = X.iloc[:int(X.shape[0]*0.80),-1]\n",
        "  #y_test = X.iloc[int(X.shape[0]*0.80):,-1]\n",
        "\n",
        "  X_train = X.iloc[1:-331,:-1]\n",
        "  X_test = X.iloc[-331:,:-1]\n",
        "  y_train = X.iloc[1:-331,-1]\n",
        "  y_test = X.iloc[-331:,-1]\n",
        "\n",
        "  y_train = y_train.values.reshape(len(y_train),1)\n",
        "  y_test = y_test.values.reshape(len(y_test),1)\n",
        "\n",
        "  ####Scaling the data after train-test split\n",
        "  sc_X = StandardScaler()\n",
        "  sc_y = StandardScaler()\n",
        "  X_train_normalized = sc_X.fit_transform(X_train)\n",
        "  X_test_normalized = sc_X.transform(X_test)\n",
        "  y_train_normalized = sc_y.fit_transform(y_train)\n",
        "  y_test_normalized = sc_y.transform(y_test)\n",
        "  \n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "  train_X = X_train_normalized.reshape((X_train_normalized.shape[0], 1, X_train_normalized.shape[1]))\n",
        "  test_X = X_test_normalized.reshape((X_test_normalized.shape[0], 1, X_test_normalized.shape[1]))\n",
        "\n",
        "  n_input = 1\n",
        "  n_features = X_train_normalized.shape[1]\n",
        "\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(70, activation='relu', input_shape=(n_input, n_features)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(1))\n",
        "  opt = keras.optimizers.Adam(lr=0.0001)\n",
        "  model.compile(optimizer=opt, loss='mse')\n",
        "  # fit model\n",
        "  \n",
        "  history = model.fit(train_X, y_train_normalized, epochs=100, batch_size=2, validation_data=(test_X, y_test_normalized), \n",
        "                      verbose=1, shuffle=False)\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  # plot history\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.plot(history.history['val_loss'], label='test')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # make a prediction\n",
        "  yhat = model.predict(test_X)\n",
        "  #print(yhat.shape)\n",
        "  test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
        "  \n",
        "  # invert scaling for forecast\n",
        "  inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
        "  inv_yhat = sc_y.inverse_transform(inv_yhat)\n",
        "  inv_yhat = inv_yhat[:,0]\n",
        "  #print(inv_yhat)\n",
        "  \n",
        "  # invert scaling for actual\n",
        "  y_test_normalized = y_test_normalized.reshape((len(y_test_normalized), 1))\n",
        "  inv_y = concatenate((y_test_normalized, test_X[:, 1:]), axis=1)\n",
        "  inv_y = sc_y.inverse_transform(inv_y)\n",
        "  inv_y = inv_y[:,0]\n",
        "  #print(inv_y)\n",
        "\n",
        "  \n",
        "  #plt.plot(inv_y, 'go', linewidth=4, label='Actual Value')\n",
        "  #plt.plot(inv_yhat,'rx--', linewidth=2, label='Predicted Value')\n",
        "  #plt.legend()\n",
        "  #plt.savefig(\"abc.png\")\n",
        "  #files.download(\"abc.png\")\n",
        "  #plt.show()\n",
        "\n",
        "  fig2 = plt.figure(figsize=(8, 6)) # The size of the figure is specified as (width, height) in inches\n",
        "\n",
        "  # lines:\n",
        "  l1 = fig2.add_subplot(111).plot(inv_y, 'co-')\n",
        "  l2 = fig2.add_subplot(111).plot(inv_yhat,'rx--')\n",
        "  fig2.add_subplot(111).legend(['Actual', 'Predicted'], loc=0)\n",
        "\n",
        "\n",
        "  # axes:\n",
        "  fig2.add_subplot(111).grid(True)\n",
        "\n",
        "  # labels:\n",
        "  fig2.add_subplot(111).set_xlabel(r\"Days\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_ylabel(r\"Global_active_power (in Kilowatts)\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_title(r\"Actual vs. Predicted for daily case\", fontsize=15)\n",
        "\n",
        "  # saving:\n",
        "  fig2.savefig(\"lstm_multivariate_days.png\", dpi=150)\n",
        "  files.download(\"lstm_multivariate_days.png\")\n",
        "  fig2.show()\n",
        "\n",
        "\n",
        "\n",
        "  print(inv_y.shape)\n",
        "\n",
        "  #print(inv_yhat.shape)\n",
        "  #print(inv_y[:])\n",
        "\n",
        "  res = np.vstack((inv_y, inv_yhat))\n",
        "  res = res.transpose()\n",
        "  DF = pd.DataFrame(res)\n",
        "  # save the dataframe as a csv file\n",
        "  DF.to_csv(\"results.csv\")\n",
        "\n",
        "  # calculate RMSE\n",
        "  rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "  print(f'Root mean square error (RMSE) is: {rmse} kilowatts')\n",
        "  mape = np.mean(np.abs((inv_y.flatten() - inv_yhat.flatten()) /inv_y.flatten())) * 100\n",
        "  print(f'Mean absolute percentage error is: {mape}')\n",
        "  mae_error = mae(inv_y.flatten(),inv_yhat.flatten())\n",
        "  print(f'Mean Absolute Error is: {mae_error} ')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfWQPsPJWYfp"
      },
      "source": [
        "X = data_response_daily.copy()\n",
        "Multivariate_LSTM(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhQpf2hSAHie"
      },
      "source": [
        "### For multi-layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBgntTdxnxZ1"
      },
      "source": [
        "def Multivariate_LSTM_2(X):\n",
        "  #Splitting the data into train and test sets. Since we are dealing with time-series data we cannot split it into randomized split. It must be a sequential split.\n",
        "  #Here we are using first 75% data for training and 25% data for testing\n",
        "  #X_train = X.iloc[:int(X.shape[0]*0.80),:]\n",
        "  #X_test = X.iloc[int(X.shape[0]*0.80):,:]\n",
        "  #y_train = X.iloc[:int(X.shape[0]*0.80),-1]\n",
        "  #y_test = X.iloc[int(X.shape[0]*0.80):,-1]\n",
        "\n",
        "  X_train = X.iloc[1:-331,:-1]\n",
        "  X_test = X.iloc[-331:,:-1]\n",
        "  y_train = X.iloc[1:-331,-1]\n",
        "  y_test = X.iloc[-331:,-1]\n",
        "\n",
        "  y_train = y_train.values.reshape(len(y_train),1)\n",
        "  y_test = y_test.values.reshape(len(y_test),1)\n",
        "\n",
        "  ####Scaling the data after train-test split\n",
        "  sc_X = StandardScaler()\n",
        "  sc_y = StandardScaler()\n",
        "  X_train_normalized = sc_X.fit_transform(X_train)\n",
        "  X_test_normalized = sc_X.transform(X_test)\n",
        "  y_train_normalized = sc_y.fit_transform(y_train)\n",
        "  y_test_normalized = sc_y.transform(y_test)\n",
        "\n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "  train_X = X_train_normalized.reshape((X_train_normalized.shape[0], 1, X_train_normalized.shape[1]))\n",
        "  test_X = X_test_normalized.reshape((X_test_normalized.shape[0], 1, X_test_normalized.shape[1]))\n",
        "\n",
        "  n_input = 1\n",
        "  n_features = X_train_normalized.shape[1]\n",
        "\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(70, activation='relu', return_sequences = True, input_shape=(n_input, n_features)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(30, activation='relu',return_sequences = True))\n",
        "  model.add(Dropout(0.1))\n",
        "  # can add more layers if required\n",
        "  model.add(Dense(1))\n",
        "  opt = keras.optimizers.Adam(lr=0.0001)\n",
        "  model.compile(optimizer=opt, loss='mse')\n",
        "  # fit model\n",
        "  history = model.fit(train_X, y_train_normalized, epochs=150, batch_size=10, validation_data=(test_X, y_test_normalized), \n",
        "                      verbose=1, shuffle=False)\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  # plot history\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.plot(history.history['val_loss'], label='test')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # make a prediction\n",
        "  yhat = model.predict(test_X)\n",
        "  yhat.shape\n",
        "  test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
        "  \n",
        "  # invert scaling for forecast\n",
        "  yhat = yhat.reshape((yhat.shape[0], yhat.shape[2]))\n",
        "  inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
        "  inv_yhat = sc_y.inverse_transform(inv_yhat)\n",
        "  inv_yhat = inv_yhat[:,0]\n",
        "  #print(inv_yhat)\n",
        "  \n",
        "  # invert scaling for actual\n",
        "  y_test_normalized = y_test_normalized.reshape((len(y_test_normalized), 1))\n",
        "  inv_y = concatenate((y_test_normalized, test_X[:, 1:]), axis=1)\n",
        "  inv_y = sc_y.inverse_transform(inv_y)\n",
        "  inv_y = inv_y[:,0]\n",
        "  #print(inv_y)\n",
        "\n",
        "\n",
        "  #plt.plot(inv_y, 'go', linewidth=4, label='Actual Value')\n",
        "  #plt.plot(inv_yhat,'rx--', linewidth=2, label='Predicted Value')\n",
        "  #plt.legend()\n",
        "  #plt.savefig(\"abc.png\")\n",
        "  #files.download(\"abc.png\")\n",
        "  #plt.show()\n",
        "\n",
        "  fig2 = plt.figure(figsize=(8, 6)) # The size of the figure is specified as (width, height) in inches\n",
        "\n",
        "  # lines:\n",
        "  l1 = fig2.add_subplot(111).plot(inv_y, 'co-')\n",
        "  l2 = fig2.add_subplot(111).plot(inv_yhat,'rx--')\n",
        "  fig2.add_subplot(111).legend(['Actual', 'Predicted'], loc=0)\n",
        "\n",
        "\n",
        "  # axes:\n",
        "  fig2.add_subplot(111).grid(True)\n",
        "\n",
        "  # labels:\n",
        "  fig2.add_subplot(111).set_xlabel(r\"Days\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_ylabel(r\"Global_active_power (in Kilowatts)\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_title(r\"Actual vs. Predicted for daily case\", fontsize=15)\n",
        "\n",
        "  # saving:\n",
        "  fig2.savefig(\"lstm_multivariate_days.png\", dpi=150)\n",
        "  files.download(\"lstm_multivariate_days.png\")\n",
        "  fig2.show()\n",
        "\n",
        "\n",
        "\n",
        "  #print(inv_y.shape)\n",
        "\n",
        "  #print(inv_yhat.shape)\n",
        "  #print(inv_y[:])\n",
        "\n",
        "  # calculate RMSE\n",
        "  rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "  print(f'Root mean square error (RMSE) is: {rmse} kilowatts')\n",
        "  mape = np.mean(np.abs((inv_y.flatten() - inv_yhat.flatten()) /inv_y.flatten())) * 100\n",
        "  print(f'Mean absolute percentage error is: {mape}')\n",
        "  mae_error = mae(inv_y.flatten(),inv_yhat.flatten())\n",
        "  print(f'Mean Absolute Error is: {mae_error} ')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXipHlZx_8r0"
      },
      "source": [
        "X = data_response_daily.copy()\n",
        "Multivariate_LSTM_2(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRMjlGV1AZMa"
      },
      "source": [
        "### For timesteps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKhH3ihfK0AJ"
      },
      "source": [
        "def timesteps(x_train,y_train,lag):\n",
        "  X = []\n",
        "  Y = []\n",
        "  num_samples = x_train.shape[0]\n",
        "\n",
        "  for i in range(lag, num_samples):\n",
        "\n",
        "    temp = x_train[i-lag:i, :]\n",
        "    #X.append(temp.reshape(1,temp.shape[0],temp.shape[1]))\n",
        "    X.append(temp)\n",
        "    Y.append(y_train[i])\n",
        "\n",
        "  X,Y = np.array(X), np.array(Y)\n",
        "  \n",
        "  return X,Y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDX0jWfbPYoX"
      },
      "source": [
        "def Multivariate_LSTM_timesteps(X,lag):\n",
        "  #Splitting the data into train and test sets. Since we are dealing with time-series data we cannot split it into randomized split. It must be a sequential split.\n",
        "  #Here we are using first 75% data for training and 25% data for testing\n",
        "  #X_train = X.iloc[:int(X.shape[0]*0.80),:]\n",
        "  #X_test = X.iloc[int(X.shape[0]*0.80):,:]\n",
        "  #y_train = X.iloc[:int(X.shape[0]*0.80),-1]\n",
        "  #y_test = X.iloc[int(X.shape[0]*0.80):,-1]\n",
        "\n",
        "  X_train = X.iloc[1:-331,:-1]\n",
        "  X_test = X.iloc[-331:,:-1]\n",
        "  y_train = X.iloc[1:-331,-1]\n",
        "  y_test = X.iloc[-331:,-1]\n",
        "\n",
        "  y_train = y_train.values.reshape(len(y_train),1)\n",
        "  y_test = y_test.values.reshape(len(y_test),1)\n",
        "\n",
        "  ####Scaling the data after train-test split\n",
        "  sc_X = StandardScaler()\n",
        "  sc_y = StandardScaler()\n",
        "  X_train_normalized = sc_X.fit_transform(X_train)\n",
        "  X_test_normalized = sc_X.transform(X_test)\n",
        "  y_train_normalized = sc_y.fit_transform(y_train)\n",
        "  y_test_normalized = sc_y.transform(y_test)\n",
        "\n",
        "  # Creating data in timesteps\n",
        "  Xt_train,Yt_train = timesteps(X_train_normalized,y_train_normalized,lag)\n",
        "  Xt_test,Yt_test = timesteps(X_test_normalized,y_test_normalized,lag)\n",
        "  \n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "  train_X = Xt_train\n",
        "  test_X = Xt_test\n",
        "\n",
        "  n_input = lag\n",
        "  n_features = train_X.shape[2]\n",
        "\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(100, activation='relu', return_sequences = True, input_shape=(n_input, n_features)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(35, activation='relu',return_sequences = True))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(1))\n",
        "  opt = keras.optimizers.Adam(lr=0.0001)\n",
        "  model.compile(optimizer=opt, loss='mse')\n",
        "  # fit model\n",
        "  history = model.fit(train_X, Yt_train, epochs=150, batch_size=40, validation_data=(test_X, Yt_test), \n",
        "                     verbose=1, shuffle=False)\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  # plot history\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.plot(history.history['val_loss'], label='test')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # make a prediction\n",
        "  yhat = model.predict(test_X)\n",
        "  #print(yhat.shape)\n",
        "  test_X = test_X[:,-1,:].reshape((test_X.shape[0], test_X.shape[2]))\n",
        "\n",
        "  # invert scaling for forecast\n",
        "  yhat = yhat[:,-1,:].reshape((yhat.shape[0], yhat.shape[2]))\n",
        "  inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
        "  inv_yhat = sc_y.inverse_transform(inv_yhat)\n",
        "  inv_yhat = inv_yhat[:,0]\n",
        "  #print(inv_yhat)\n",
        "\n",
        "  # invert scaling for actual\n",
        "  y_test_normalized = y_test_normalized.reshape((len(y_test_normalized), 1))\n",
        "  inv_y = concatenate((Yt_test, test_X[:, 1:]), axis=1)\n",
        "  inv_y = sc_y.inverse_transform(inv_y)\n",
        "  inv_y = inv_y[:,0]\n",
        "  #print(inv_y)\n",
        "\n",
        "\n",
        "  #plt.plot(inv_y, 'go', linewidth=4, label='Actual Value')\n",
        "  #plt.plot(inv_yhat,'rx--', linewidth=2, label='Predicted Value')\n",
        "  #plt.legend()\n",
        "  #plt.savefig(\"abc.png\")\n",
        "  #files.download(\"abc.png\")\n",
        "  #plt.show()\n",
        "\n",
        "  fig2 = plt.figure(figsize=(8, 6)) # The size of the figure is specified as (width, height) in inches\n",
        "\n",
        "  # lines:\n",
        "  l1 = fig2.add_subplot(111).plot(inv_y, 'co-')\n",
        "  l2 = fig2.add_subplot(111).plot(inv_yhat,'rx--')\n",
        "  fig2.add_subplot(111).legend(['Actual', 'Predicted'], loc=0)\n",
        "\n",
        "\n",
        "  # axes:\n",
        "  fig2.add_subplot(111).grid(True)\n",
        "\n",
        "  # labels:\n",
        "  fig2.add_subplot(111).set_xlabel(r\"Days\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_ylabel(r\"Global_active_power (in Kilowatts)\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_title(r\"Actual vs. Predicted for daily case\", fontsize=15)\n",
        "\n",
        "  # saving:\n",
        "  fig2.savefig(\"lstm_multivariate_days.png\", dpi=150)\n",
        "  files.download(\"lstm_multivariate_days.png\")\n",
        "  fig2.show()\n",
        "\n",
        "\n",
        "\n",
        "  #print(inv_y.shape)\n",
        "\n",
        "  #print(inv_yhat.shape)\n",
        "  #print(inv_y[:])\n",
        "\n",
        "  # calculate RMSE\n",
        "  rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "  print(f'Root mean square error (RMSE) is: {rmse} kilowatts')\n",
        "  mape = np.mean(np.abs((inv_y.flatten() - inv_yhat.flatten()) /inv_y.flatten())) * 100\n",
        "  print(f'Mean absolute percentage error is: {mape}')\n",
        "  mae_error = mae(inv_y.flatten(),inv_yhat.flatten())\n",
        "  print(f'Mean Absolute Error is: {mae_error} ')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkR4vh-NQgXn"
      },
      "source": [
        "X = data_response_daily.copy()\n",
        "Multivariate_LSTM_timesteps(X,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RddBFVFBqLU"
      },
      "source": [
        "## For weeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR2a1PkGKWZK"
      },
      "source": [
        "### For 1 layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR5SNn61BsFT"
      },
      "source": [
        "dataset = weather_response_weekly\n",
        "#train,test = dataset.values[:-48],dataset.values[-48:]\n",
        "#dataset[1:-331]\n",
        "dataset[1:-48]\n",
        "dataset.iloc[1:-48]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZpFteUkMdBJ"
      },
      "source": [
        "def Multivariate_LSTM(X):\n",
        "  #Splitting the data into train and test sets. Since we are dealing with time-series data we cannot split it into randomized split. It must be a sequential split.\n",
        "  #Here we are using first 75% data for training and 25% data for testing\n",
        "\n",
        "  X_train = X.iloc[1:-48,:-1]\n",
        "  X_test = X.iloc[-48:,:-1]\n",
        "  y_train = X.iloc[1:-48,-1]\n",
        "  y_test = X.iloc[-48:,-1]\n",
        "\n",
        "  y_train = y_train.values.reshape(len(y_train),1)\n",
        "  y_test = y_test.values.reshape(len(y_test),1)\n",
        "\n",
        "  ####Scaling the data after train-test split\n",
        "  sc_X = StandardScaler()\n",
        "  sc_y = StandardScaler()\n",
        "  X_train_normalized = sc_X.fit_transform(X_train)\n",
        "  X_test_normalized = sc_X.transform(X_test)\n",
        "  y_train_normalized = sc_y.fit_transform(y_train)\n",
        "  y_test_normalized = sc_y.transform(y_test)\n",
        "  \n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "  train_X = X_train_normalized.reshape((X_train_normalized.shape[0], 1, X_train_normalized.shape[1]))\n",
        "  test_X = X_test_normalized.reshape((X_test_normalized.shape[0], 1, X_test_normalized.shape[1]))\n",
        "\n",
        "  n_input = 1\n",
        "  n_features = X_train_normalized.shape[1]\n",
        "\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(60, activation='relu', input_shape=(n_input, n_features)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(optimizer='adam', loss='mse')\n",
        "  # fit model\n",
        "  history = model.fit(train_X, y_train_normalized, epochs=100, batch_size= 5, validation_data=(test_X, y_test_normalized), \n",
        "                     verbose=1, shuffle=False)\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  # plot history\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.plot(history.history['val_loss'], label='test')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # make a prediction\n",
        "  yhat = model.predict(test_X)\n",
        "  test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
        "  \n",
        "  # invert scaling for forecast\n",
        "  inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
        "  inv_yhat = sc_y.inverse_transform(inv_yhat)\n",
        "  inv_yhat = inv_yhat[:,0]\n",
        "  #print(inv_yhat)\n",
        "  \n",
        "  # invert scaling for actual\n",
        "  y_test_normalized = y_test_normalized.reshape((len(y_test_normalized), 1))\n",
        "  inv_y = concatenate((y_test_normalized, test_X[:, 1:]), axis=1)\n",
        "  inv_y = sc_y.inverse_transform(inv_y)\n",
        "  inv_y = inv_y[:,0]\n",
        "  #print(inv_y)\n",
        "\n",
        "  \n",
        "  #plt.plot(inv_y, 'go', linewidth=4, label='Actual Value')\n",
        "  #plt.plot(inv_yhat,'rx--', linewidth=2, label='Predicted Value')\n",
        "  #plt.legend()\n",
        "  #plt.savefig(\"abc.png\")\n",
        "  #files.download(\"abc.png\")\n",
        "  #plt.show()\n",
        "\n",
        "  fig2 = plt.figure(figsize=(8, 6)) # The size of the figure is specified as (width, height) in inches\n",
        "\n",
        "  # lines:\n",
        "  l1 = fig2.add_subplot(111).plot(inv_y, 'co-')\n",
        "  l2 = fig2.add_subplot(111).plot(inv_yhat,'rx--')\n",
        "  fig2.add_subplot(111).legend(['Actual', 'Predicted'], loc=0)\n",
        "\n",
        "\n",
        "  # axes:\n",
        "  fig2.add_subplot(111).grid(True)\n",
        "\n",
        "  # labels:\n",
        "  fig2.add_subplot(111).set_xlabel(r\"Weeks\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_ylabel(r\"Global_active_power (in Kilowatts)\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_title(r\"Actual vs. Predicted for Weekly case\", fontsize=15)\n",
        "\n",
        "  # saving:\n",
        "  fig2.savefig(\"lstm_multivariate_weeks.png\", dpi=150)\n",
        "  files.download(\"lstm_multivariate_weeks.png\")\n",
        "  fig2.show()\n",
        "\n",
        "\n",
        "\n",
        "  #print(inv_y.shape)\n",
        "\n",
        "  #print(inv_yhat.shape)\n",
        "  #print(inv_y[:])\n",
        "\n",
        "  res = np.vstack((inv_y, inv_yhat))\n",
        "  res = res.transpose()\n",
        "  DF = pd.DataFrame(res)\n",
        "  # save the dataframe as a csv file\n",
        "  DF.to_csv(\"results.csv\")\n",
        "\n",
        "  # calculate RMSE\n",
        "  rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "  print(f'Root mean square error (RMSE) is: {rmse} kilowatts')\n",
        "  mape = np.mean(np.abs((inv_y.flatten() - inv_yhat.flatten()) /inv_y.flatten())) * 100\n",
        "  print(f'Mean absolute percentage error is: {mape}')\n",
        "  mae_error = mae(inv_y.flatten(),inv_yhat.flatten())\n",
        "  print(f'Mean Absolute Error is: {mae_error} ')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mw7KD4kMdPe"
      },
      "source": [
        "X = data_response_weekly.copy()\n",
        "Multivariate_LSTM(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dsVg3_jKnWw"
      },
      "source": [
        "### For multi-layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deUkwgiOKqJ1"
      },
      "source": [
        "def Multivariate_LSTM_2(X):\n",
        "  #Splitting the data into train and test sets. Since we are dealing with time-series data we cannot split it into randomized split. It must be a sequential split.\n",
        "  #Here we are using first 75% data for training and 25% data for testing\n",
        "  #X_train = X.iloc[:int(X.shape[0]*0.80),:]\n",
        "  #X_test = X.iloc[int(X.shape[0]*0.80):,:]\n",
        "  #y_train = X.iloc[:int(X.shape[0]*0.80),-1]\n",
        "  #y_test = X.iloc[int(X.shape[0]*0.80):,-1]\n",
        "\n",
        "  X_train = X.iloc[1:-48,:-1]\n",
        "  X_test = X.iloc[-48:,:-1]\n",
        "  y_train = X.iloc[1:-48,-1]\n",
        "  y_test = X.iloc[-48:,-1]\n",
        "\n",
        "  y_train = y_train.values.reshape(len(y_train),1)\n",
        "  y_test = y_test.values.reshape(len(y_test),1)\n",
        "\n",
        "  ####Scaling the data after train-test split\n",
        "  sc_X = StandardScaler()\n",
        "  sc_y = StandardScaler()\n",
        "  X_train_normalized = sc_X.fit_transform(X_train)\n",
        "  X_test_normalized = sc_X.transform(X_test)\n",
        "  y_train_normalized = sc_y.fit_transform(y_train)\n",
        "  y_test_normalized = sc_y.transform(y_test)\n",
        "\n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "  train_X = X_train_normalized.reshape((X_train_normalized.shape[0], 1, X_train_normalized.shape[1]))\n",
        "  test_X = X_test_normalized.reshape((X_test_normalized.shape[0], 1, X_test_normalized.shape[1]))\n",
        "\n",
        "  n_input = 1\n",
        "  n_features = X_train_normalized.shape[1]\n",
        "\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(70, activation='relu', return_sequences = True, input_shape=(n_input, n_features)))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(LSTM(30, activation='relu',return_sequences = True))\n",
        "  model.add(Dropout(0.1))\n",
        "  # can add more layers if required\n",
        "  model.add(Dense(1))\n",
        "  opt = keras.optimizers.Adam(lr=0.0001)\n",
        "  model.compile(optimizer=opt, loss='mse')\n",
        "  # fit model\n",
        "  history = model.fit(train_X, y_train_normalized, epochs=150, batch_size=3, validation_data=(test_X, y_test_normalized), \n",
        "                      verbose=1, shuffle=False)\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  # plot history\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.plot(history.history['val_loss'], label='test')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # make a prediction\n",
        "  yhat = model.predict(test_X)\n",
        "  yhat.shape\n",
        "  test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
        "  \n",
        "  # invert scaling for forecast\n",
        "  yhat = yhat.reshape((yhat.shape[0], yhat.shape[2]))\n",
        "  inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
        "  inv_yhat = sc_y.inverse_transform(inv_yhat)\n",
        "  inv_yhat = inv_yhat[:,0]\n",
        "  #print(inv_yhat)\n",
        "  \n",
        "  # invert scaling for actual\n",
        "  y_test_normalized = y_test_normalized.reshape((len(y_test_normalized), 1))\n",
        "  inv_y = concatenate((y_test_normalized, test_X[:, 1:]), axis=1)\n",
        "  inv_y = sc_y.inverse_transform(inv_y)\n",
        "  inv_y = inv_y[:,0]\n",
        "  #print(inv_y)\n",
        "\n",
        "\n",
        "  #plt.plot(inv_y, 'go', linewidth=4, label='Actual Value')\n",
        "  #plt.plot(inv_yhat,'rx--', linewidth=2, label='Predicted Value')\n",
        "  #plt.legend()\n",
        "  #plt.savefig(\"abc.png\")\n",
        "  #files.download(\"abc.png\")\n",
        "  #plt.show()\n",
        "\n",
        "  fig2 = plt.figure(figsize=(8, 6)) # The size of the figure is specified as (width, height) in inches\n",
        "\n",
        "  # lines:\n",
        "  l1 = fig2.add_subplot(111).plot(inv_y, 'co-')\n",
        "  l2 = fig2.add_subplot(111).plot(inv_yhat,'rx--')\n",
        "  fig2.add_subplot(111).legend(['Actual', 'Predicted'], loc=0)\n",
        "\n",
        "\n",
        "  # axes:\n",
        "  fig2.add_subplot(111).grid(True)\n",
        "\n",
        "  # labels:\n",
        "  fig2.add_subplot(111).set_xlabel(r\"Days\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_ylabel(r\"Global_active_power (in Kilowatts)\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_title(r\"Actual vs. Predicted for daily case\", fontsize=15)\n",
        "\n",
        "  # saving:\n",
        "  fig2.savefig(\"lstm_multivariate_days.png\", dpi=150)\n",
        "  files.download(\"lstm_multivariate_days.png\")\n",
        "  fig2.show()\n",
        "\n",
        "\n",
        "\n",
        "  #print(inv_y.shape)\n",
        "\n",
        "  #print(inv_yhat.shape)\n",
        "  #print(inv_y[:])\n",
        "\n",
        "  # calculate RMSE\n",
        "  rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "  print(f'Root mean square error (RMSE) is: {rmse} kilowatts')\n",
        "  mape = np.mean(np.abs((inv_y.flatten() - inv_yhat.flatten()) /inv_y.flatten())) * 100\n",
        "  print(f'Mean absolute percentage error is: {mape}')\n",
        "  mae_error = mae(inv_y.flatten(),inv_yhat.flatten())\n",
        "  print(f'Mean Absolute Error is: {mae_error} ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRWmxSw9K_hj"
      },
      "source": [
        "X = data_response_weekly.copy()\n",
        "Multivariate_LSTM_2(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ne065eaLEn5"
      },
      "source": [
        "### For timesteps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CwhmUs_LIzZ"
      },
      "source": [
        "def timesteps(x_train,y_train,lag):\n",
        "  X = []\n",
        "  Y = []\n",
        "  num_samples = x_train.shape[0]\n",
        "\n",
        "  for i in range(lag, num_samples):\n",
        "\n",
        "    temp = x_train[i-lag:i, :]\n",
        "    #X.append(temp.reshape(1,temp.shape[0],temp.shape[1]))\n",
        "    X.append(temp)\n",
        "    Y.append(y_train[i])\n",
        "\n",
        "  X,Y = np.array(X), np.array(Y)\n",
        "  \n",
        "  return X,Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRWcmFK_LJyI"
      },
      "source": [
        "def Multivariate_LSTM_timesteps(X,lag):\n",
        "  #Splitting the data into train and test sets. Since we are dealing with time-series data we cannot split it into randomized split. It must be a sequential split.\n",
        "  #Here we are using first 75% data for training and 25% data for testing\n",
        "  #X_train = X.iloc[:int(X.shape[0]*0.80),:]\n",
        "  #X_test = X.iloc[int(X.shape[0]*0.80):,:]\n",
        "  #y_train = X.iloc[:int(X.shape[0]*0.80),-1]\n",
        "  #y_test = X.iloc[int(X.shape[0]*0.80):,-1]\n",
        "\n",
        "  X_train = X.iloc[1:-48,:-1]\n",
        "  X_test = X.iloc[-48:,:-1]\n",
        "  y_train = X.iloc[1:-48,-1]\n",
        "  y_test = X.iloc[-48:,-1]\n",
        "\n",
        "  y_train = y_train.values.reshape(len(y_train),1)\n",
        "  y_test = y_test.values.reshape(len(y_test),1)\n",
        "\n",
        "  ####Scaling the data after train-test split\n",
        "  sc_X = StandardScaler()\n",
        "  sc_y = StandardScaler()\n",
        "  X_train_normalized = sc_X.fit_transform(X_train)\n",
        "  X_test_normalized = sc_X.transform(X_test)\n",
        "  y_train_normalized = sc_y.fit_transform(y_train)\n",
        "  y_test_normalized = sc_y.transform(y_test)\n",
        "\n",
        "  # Creating data in timesteps\n",
        "  Xt_train,Yt_train = timesteps(X_train_normalized,y_train_normalized,lag)\n",
        "  Xt_test,Yt_test = timesteps(X_test_normalized,y_test_normalized,lag)\n",
        "  \n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "  train_X = Xt_train\n",
        "  test_X = Xt_test\n",
        "\n",
        "  n_input = lag\n",
        "  n_features = train_X.shape[2]\n",
        "\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(100, activation='relu', return_sequences = True, input_shape=(n_input, n_features)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(35, activation='relu',return_sequences = True))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(1))\n",
        "  opt = keras.optimizers.Adam(lr=0.0001)\n",
        "  model.compile(optimizer=opt, loss='mse')\n",
        "  # fit model\n",
        "  history = model.fit(train_X, Yt_train, epochs=70, batch_size=3, validation_data=(test_X, Yt_test), \n",
        "                     verbose=1, shuffle=False)\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  # plot history\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.plot(history.history['val_loss'], label='test')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # make a prediction\n",
        "  yhat = model.predict(test_X)\n",
        "  #print(yhat.shape)\n",
        "  test_X = test_X[:,-1,:].reshape((test_X.shape[0], test_X.shape[2]))\n",
        "\n",
        "  # invert scaling for forecast\n",
        "  yhat = yhat[:,-1,:].reshape((yhat.shape[0], yhat.shape[2]))\n",
        "  inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
        "  inv_yhat = sc_y.inverse_transform(inv_yhat)\n",
        "  inv_yhat = inv_yhat[:,0]\n",
        "  #print(inv_yhat)\n",
        "\n",
        "  # invert scaling for actual\n",
        "  y_test_normalized = y_test_normalized.reshape((len(y_test_normalized), 1))\n",
        "  inv_y = concatenate((Yt_test, test_X[:, 1:]), axis=1)\n",
        "  inv_y = sc_y.inverse_transform(inv_y)\n",
        "  inv_y = inv_y[:,0]\n",
        "\n",
        "\n",
        "  #plt.plot(inv_y, 'go', linewidth=4, label='Actual Value')\n",
        "  #plt.plot(inv_yhat,'rx--', linewidth=2, label='Predicted Value')\n",
        "  #plt.legend()\n",
        "  #plt.savefig(\"abc.png\")\n",
        "  #files.download(\"abc.png\")\n",
        "  #plt.show()\n",
        "\n",
        "  fig2 = plt.figure(figsize=(8, 6)) # The size of the figure is specified as (width, height) in inches\n",
        "\n",
        "  # lines:\n",
        "  l1 = fig2.add_subplot(111).plot(inv_y, 'co-')\n",
        "  l2 = fig2.add_subplot(111).plot(inv_yhat,'rx--')\n",
        "  fig2.add_subplot(111).legend(['Actual', 'Predicted'], loc=0)\n",
        "\n",
        "\n",
        "  # axes:\n",
        "  fig2.add_subplot(111).grid(True)\n",
        "\n",
        "  # labels:\n",
        "  fig2.add_subplot(111).set_xlabel(r\"Days\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_ylabel(r\"Global_active_power (in Kilowatts)\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_title(r\"Actual vs. Predicted for daily case\", fontsize=15)\n",
        "\n",
        "  # saving:\n",
        "  fig2.savefig(\"lstm_multivariate_days.png\", dpi=150)\n",
        "  files.download(\"lstm_multivariate_days.png\")\n",
        "  fig2.show()\n",
        "\n",
        "\n",
        "\n",
        "  #print(inv_y.shape)\n",
        "\n",
        "  #print(inv_yhat.shape)\n",
        "  #print(inv_y[:])\n",
        "\n",
        "  # calculate RMSE\n",
        "  rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "  print(f'Root mean square error (RMSE) is: {rmse} kilowatts')\n",
        "  mape = np.mean(np.abs((inv_y.flatten() - inv_yhat.flatten()) /inv_y.flatten())) * 100\n",
        "  print(f'Mean absolute percentage error is: {mape}')\n",
        "  mae_error = mae(inv_y.flatten(),inv_yhat.flatten())\n",
        "  print(f'Mean Absolute Error is: {mae_error} ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXNdK6SyLJp9"
      },
      "source": [
        "X = data_response_weekly.copy()\n",
        "Multivariate_LSTM_timesteps(X,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akEGdaYKN6QC"
      },
      "source": [
        "## For months"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDHmb8HYLfvk"
      },
      "source": [
        "### For 1 layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJi4sxrmN-eH"
      },
      "source": [
        "dataset = weather_response_monthly\n",
        "#train,test = dataset.values[:-48],dataset.values[-48:]\n",
        "#dataset[1:-331]\n",
        "dataset[1:-11]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzfBtjRaOF-F"
      },
      "source": [
        "def Multivariate_LSTM(X):\n",
        "  #Splitting the data into train and test sets. Since we are dealing with time-series data we cannot split it into randomized split. It must be a sequential split.\n",
        "  #Here we are using first 75% data for training and 25% data for testing\n",
        "\n",
        "  X_train = X.iloc[1:-11,:-1]\n",
        "  X_test = X.iloc[-11:,:-1]\n",
        "  y_train = X.iloc[1:-11,-1]\n",
        "  y_test = X.iloc[-11:,-1]\n",
        "\n",
        "  y_train = y_train.values.reshape(len(y_train),1)\n",
        "  y_test = y_test.values.reshape(len(y_test),1)\n",
        "\n",
        "  ####Scaling the data after train-test split\n",
        "  sc_X = StandardScaler()\n",
        "  sc_y = StandardScaler()\n",
        "  X_train_normalized = sc_X.fit_transform(X_train)\n",
        "  X_test_normalized = sc_X.transform(X_test)\n",
        "  y_train_normalized = sc_y.fit_transform(y_train)\n",
        "  y_test_normalized = sc_y.transform(y_test)\n",
        "  \n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "  train_X = X_train_normalized.reshape((X_train_normalized.shape[0], 1, X_train_normalized.shape[1]))\n",
        "  test_X = X_test_normalized.reshape((X_test_normalized.shape[0], 1, X_test_normalized.shape[1]))\n",
        "\n",
        "  n_input = 1\n",
        "  n_features = X_train_normalized.shape[1]\n",
        "\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(50, activation='relu', input_shape=(n_input, n_features)))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(optimizer='adam', loss='mse')\n",
        "  # fit model\n",
        "  history = model.fit(train_X, y_train_normalized, epochs=100, batch_size=2, validation_data=(test_X, y_test_normalized), \n",
        "                     verbose=1, shuffle=False)\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  # plot history\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.plot(history.history['val_loss'], label='test')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # make a prediction\n",
        "  yhat = model.predict(test_X)\n",
        "  test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
        "  # invert scaling for forecast\n",
        "  inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
        "  inv_yhat = sc_y.inverse_transform(inv_yhat)\n",
        "  inv_yhat = inv_yhat[:,0]\n",
        "  print(inv_yhat)\n",
        "  # invert scaling for actual\n",
        "  y_test_normalized = y_test_normalized.reshape((len(y_test_normalized), 1))\n",
        "  inv_y = concatenate((y_test_normalized, test_X[:, 1:]), axis=1)\n",
        "  inv_y = sc_y.inverse_transform(inv_y)\n",
        "  inv_y = inv_y[:,0]\n",
        "  print(inv_y)\n",
        "\n",
        "  \n",
        "  #plt.plot(inv_y, 'go', linewidth=4, label='Actual Value')\n",
        "  #plt.plot(inv_yhat,'rx--', linewidth=2, label='Predicted Value')\n",
        "  #plt.legend()\n",
        "  #plt.savefig(\"abc.png\")\n",
        "  #files.download(\"abc.png\")\n",
        "  #plt.show()\n",
        "\n",
        "  fig2 = plt.figure(figsize=(8, 6)) # The size of the figure is specified as (width, height) in inches\n",
        "\n",
        "  # lines:\n",
        "  l1 = fig2.add_subplot(111).plot(inv_y, 'co-')\n",
        "  l2 = fig2.add_subplot(111).plot(inv_yhat,'rx--')\n",
        "  fig2.add_subplot(111).legend(['Actual', 'Predicted'], loc=0)\n",
        "\n",
        "\n",
        "  # axes:\n",
        "  fig2.add_subplot(111).grid(True)\n",
        "\n",
        "  # labels:\n",
        "  fig2.add_subplot(111).set_xlabel(r\"Months\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_ylabel(r\"Global_active_power (in Kilowatts)\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_title(r\"Actual vs. Predicted for Monthly case\", fontsize=15)\n",
        "\n",
        "  # saving:\n",
        "  fig2.savefig(\"lstm_multivariate_monthly.png\", dpi=150)\n",
        "  files.download(\"lstm_multivariate_monthly.png\")\n",
        "  fig2.show()\n",
        "\n",
        "  #print(inv_y.shape)\n",
        "\n",
        "  #print(inv_yhat.shape)\n",
        "  #print(inv_y[:])\n",
        "\n",
        "  res = np.vstack((inv_y, inv_yhat))\n",
        "  res = res.transpose()\n",
        "  DF = pd.DataFrame(res)\n",
        "  # save the dataframe as a csv file\n",
        "  DF.to_csv(\"results.csv\")\n",
        "\n",
        "  # calculate RMSE\n",
        "  rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "  print(f'Root mean square error (RMSE) is: {rmse} kilowatts')\n",
        "  mape = np.mean(np.abs((inv_y.flatten() - inv_yhat.flatten()) /inv_y.flatten())) * 100\n",
        "  print(f'Mean absolute percentage error is: {mape}')\n",
        "  mae_error = mae(inv_y.flatten(),inv_yhat.flatten())\n",
        "  print(f'Mean Absolute Error is: {mae_error} ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wW1X58UOG41"
      },
      "source": [
        "X = data_response_monthly.copy()\n",
        "Multivariate_LSTM(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rart0wRLrDa"
      },
      "source": [
        "### For multi-layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrl2FR7pLvqy"
      },
      "source": [
        "def Multivariate_LSTM_2(X):\n",
        "  #Splitting the data into train and test sets. Since we are dealing with time-series data we cannot split it into randomized split. It must be a sequential split.\n",
        "  # Here we are using first 3 years data for training and 1 year data for testing\n",
        "  \n",
        "  #X_train = X.iloc[:int(X.shape[0]*0.80),:]\n",
        "  #X_test = X.iloc[int(X.shape[0]*0.80):,:]\n",
        "  #y_train = X.iloc[:int(X.shape[0]*0.80),-1]\n",
        "  #y_test = X.iloc[int(X.shape[0]*0.80):,-1]\n",
        "\n",
        "  X_train = X.iloc[1:-11,:-1]\n",
        "  X_test = X.iloc[-11:,:-1]\n",
        "  y_train = X.iloc[1:-11,-1]\n",
        "  y_test = X.iloc[-11:,-1]\n",
        "\n",
        "  y_train = y_train.values.reshape(len(y_train),1)\n",
        "  y_test = y_test.values.reshape(len(y_test),1)\n",
        "\n",
        "  ####Scaling the data after train-test split\n",
        "  sc_X = StandardScaler()\n",
        "  sc_y = StandardScaler()\n",
        "  X_train_normalized = sc_X.fit_transform(X_train)\n",
        "  X_test_normalized = sc_X.transform(X_test)\n",
        "  y_train_normalized = sc_y.fit_transform(y_train)\n",
        "  y_test_normalized = sc_y.transform(y_test)\n",
        "\n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "  train_X = X_train_normalized.reshape((X_train_normalized.shape[0], 1, X_train_normalized.shape[1]))\n",
        "  test_X = X_test_normalized.reshape((X_test_normalized.shape[0], 1, X_test_normalized.shape[1]))\n",
        "\n",
        "  n_input = 1\n",
        "  n_features = X_train_normalized.shape[1]\n",
        "\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(50, activation='relu', return_sequences = True, input_shape=(n_input, n_features)))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(LSTM(30, activation='relu',return_sequences = True))\n",
        "  model.add(Dropout(0.1))\n",
        "  # can add more layers if required\n",
        "  model.add(Dense(1))\n",
        "  opt = keras.optimizers.Adam(lr=0.0001)\n",
        "  model.compile(optimizer=opt, loss='mse')\n",
        "  # fit model\n",
        "  history = model.fit(train_X, y_train_normalized, epochs=150, batch_size=2, validation_data=(test_X, y_test_normalized), \n",
        "                      verbose=1, shuffle=False)\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  # plot history\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.plot(history.history['val_loss'], label='test')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # make a prediction\n",
        "  yhat = model.predict(test_X)\n",
        "  yhat.shape\n",
        "  test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
        "  \n",
        "  # invert scaling for forecast\n",
        "  yhat = yhat.reshape((yhat.shape[0], yhat.shape[2]))\n",
        "  inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
        "  inv_yhat = sc_y.inverse_transform(inv_yhat)\n",
        "  inv_yhat = inv_yhat[:,0]\n",
        "  #print(inv_yhat)\n",
        "  \n",
        "  # invert scaling for actual\n",
        "  y_test_normalized = y_test_normalized.reshape((len(y_test_normalized), 1))\n",
        "  inv_y = concatenate((y_test_normalized, test_X[:, 1:]), axis=1)\n",
        "  inv_y = sc_y.inverse_transform(inv_y)\n",
        "  inv_y = inv_y[:,0]\n",
        "  #print(inv_y)\n",
        "\n",
        "\n",
        "  #plt.plot(inv_y, 'go', linewidth=4, label='Actual Value')\n",
        "  #plt.plot(inv_yhat,'rx--', linewidth=2, label='Predicted Value')\n",
        "  #plt.legend()\n",
        "  #plt.savefig(\"abc.png\")\n",
        "  #files.download(\"abc.png\")\n",
        "  #plt.show()\n",
        "\n",
        "  fig2 = plt.figure(figsize=(8, 6)) # The size of the figure is specified as (width, height) in inches\n",
        "\n",
        "  # lines:\n",
        "  l1 = fig2.add_subplot(111).plot(inv_y, 'co-')\n",
        "  l2 = fig2.add_subplot(111).plot(inv_yhat,'rx--')\n",
        "  fig2.add_subplot(111).legend(['Actual', 'Predicted'], loc=0)\n",
        "\n",
        "\n",
        "  # axes:\n",
        "  fig2.add_subplot(111).grid(True)\n",
        "\n",
        "  # labels:\n",
        "  fig2.add_subplot(111).set_xlabel(r\"Days\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_ylabel(r\"Global_active_power (in Kilowatts)\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_title(r\"Actual vs. Predicted for daily case\", fontsize=15)\n",
        "\n",
        "  # saving:\n",
        "  fig2.savefig(\"lstm_multivariate_days.png\", dpi=150)\n",
        "  files.download(\"lstm_multivariate_days.png\")\n",
        "  fig2.show()\n",
        "\n",
        "\n",
        "\n",
        "  #print(inv_y.shape)\n",
        "\n",
        "  #print(inv_yhat.shape)\n",
        "  #print(inv_y[:])\n",
        "\n",
        "  # calculate RMSE\n",
        "  rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "  print(f'Root mean square error (RMSE) is: {rmse} kilowatts')\n",
        "  mape = np.mean(np.abs((inv_y.flatten() - inv_yhat.flatten()) /inv_y.flatten())) * 100\n",
        "  print(f'Mean absolute percentage error is: {mape}')\n",
        "  mae_error = mae(inv_y.flatten(),inv_yhat.flatten())\n",
        "  print(f'Mean Absolute Error is: {mae_error} ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63zZ-OSqLvoD"
      },
      "source": [
        "X = data_response_monthly.copy()\n",
        "Multivariate_LSTM_2(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_poO6xIpLxBW"
      },
      "source": [
        "### For timesteps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOLk_ar6LyiR"
      },
      "source": [
        "def timesteps(x_train,y_train,lag):\n",
        "  X = []\n",
        "  Y = []\n",
        "  num_samples = x_train.shape[0]\n",
        "\n",
        "  for i in range(lag, num_samples):\n",
        "\n",
        "    temp = x_train[i-lag:i, :]\n",
        "    #X.append(temp.reshape(1,temp.shape[0],temp.shape[1]))\n",
        "    X.append(temp)\n",
        "    Y.append(y_train[i])\n",
        "\n",
        "  X,Y = np.array(X), np.array(Y)\n",
        "  \n",
        "  return X,Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhixo7GgLzVe"
      },
      "source": [
        "def Multivariate_LSTM_timesteps(X,lag):\n",
        "  #Splitting the data into train and test sets. Since we are dealing with time-series data we cannot split it into randomized split. It must be a sequential split.\n",
        "  #Here we are using first 75% data for training and 25% data for testing\n",
        "  #X_train = X.iloc[:int(X.shape[0]*0.80),:]\n",
        "  #X_test = X.iloc[int(X.shape[0]*0.80):,:]\n",
        "  #y_train = X.iloc[:int(X.shape[0]*0.80),-1]\n",
        "  #y_test = X.iloc[int(X.shape[0]*0.80):,-1]\n",
        "\n",
        "  X_train = X.iloc[1:-11,:-1]\n",
        "  X_test = X.iloc[-11:,:-1]\n",
        "  y_train = X.iloc[1:-11,-1]\n",
        "  y_test = X.iloc[-11:,-1]\n",
        "\n",
        "  y_train = y_train.values.reshape(len(y_train),1)\n",
        "  y_test = y_test.values.reshape(len(y_test),1)\n",
        "\n",
        "  ####Scaling the data after train-test split\n",
        "  sc_X = StandardScaler()\n",
        "  sc_y = StandardScaler()\n",
        "  X_train_normalized = sc_X.fit_transform(X_train)\n",
        "  X_test_normalized = sc_X.transform(X_test)\n",
        "  y_train_normalized = sc_y.fit_transform(y_train)\n",
        "  y_test_normalized = sc_y.transform(y_test)\n",
        "\n",
        "  # Creating data in timesteps\n",
        "  Xt_train,Yt_train = timesteps(X_train_normalized,y_train_normalized,lag)\n",
        "  Xt_test,Yt_test = timesteps(X_test_normalized,y_test_normalized,lag)\n",
        "  \n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "  train_X = Xt_train\n",
        "  test_X = Xt_test\n",
        "\n",
        "  n_input = lag\n",
        "  n_features = train_X.shape[2]\n",
        "\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(100, activation='relu', return_sequences = True, input_shape=(n_input, n_features)))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(LSTM(35, activation='relu',return_sequences = True))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(1))\n",
        "  opt = keras.optimizers.Adam(lr=0.0001)\n",
        "  model.compile(optimizer=opt, loss='mse')\n",
        "  # fit model\n",
        "  history = model.fit(train_X, Yt_train, epochs=60, batch_size=2, validation_data=(test_X, Yt_test), \n",
        "                     verbose=1, shuffle=False)\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  # plot history\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.plot(history.history['val_loss'], label='test')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # make a prediction\n",
        "  yhat = model.predict(test_X)\n",
        "  #print(yhat.shape)\n",
        "  test_X = test_X[:,-1,:].reshape((test_X.shape[0], test_X.shape[2]))\n",
        "\n",
        "  # invert scaling for forecast\n",
        "  yhat = yhat[:,-1,:].reshape((yhat.shape[0], yhat.shape[2]))\n",
        "  inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
        "  inv_yhat = sc_y.inverse_transform(inv_yhat)\n",
        "  inv_yhat = inv_yhat[:,0]\n",
        "  #print(inv_yhat)\n",
        "\n",
        "  # invert scaling for actual\n",
        "  y_test_normalized = y_test_normalized.reshape((len(y_test_normalized), 1))\n",
        "  inv_y = concatenate((Yt_test, test_X[:, 1:]), axis=1)\n",
        "  inv_y = sc_y.inverse_transform(inv_y)\n",
        "  inv_y = inv_y[:,0]\n",
        "\n",
        "\n",
        "  #plt.plot(inv_y, 'go', linewidth=4, label='Actual Value')\n",
        "  #plt.plot(inv_yhat,'rx--', linewidth=2, label='Predicted Value')\n",
        "  #plt.legend()\n",
        "  #plt.savefig(\"abc.png\")\n",
        "  #files.download(\"abc.png\")\n",
        "  #plt.show()\n",
        "\n",
        "  fig2 = plt.figure(figsize=(8, 6)) # The size of the figure is specified as (width, height) in inches\n",
        "\n",
        "  # lines:\n",
        "  l1 = fig2.add_subplot(111).plot(inv_y, 'co-')\n",
        "  l2 = fig2.add_subplot(111).plot(inv_yhat,'rx--')\n",
        "  fig2.add_subplot(111).legend(['Actual', 'Predicted'], loc=0)\n",
        "\n",
        "\n",
        "  # axes:\n",
        "  fig2.add_subplot(111).grid(True)\n",
        "\n",
        "  # labels:\n",
        "  fig2.add_subplot(111).set_xlabel(r\"Days\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_ylabel(r\"Global_active_power (in Kilowatts)\", fontsize=15)\n",
        "  fig2.add_subplot(111).set_title(r\"Actual vs. Predicted for daily case\", fontsize=15)\n",
        "\n",
        "  # saving:\n",
        "  fig2.savefig(\"lstm_multivariate_days.png\", dpi=150)\n",
        "  files.download(\"lstm_multivariate_days.png\")\n",
        "  fig2.show()\n",
        "\n",
        "\n",
        "\n",
        "  #print(inv_y.shape)\n",
        "\n",
        "  #print(inv_yhat.shape)\n",
        "  #print(inv_y[:])\n",
        "\n",
        "  # calculate RMSE\n",
        "  rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "  print(f'Root mean square error (RMSE) is: {rmse} kilowatts')\n",
        "  mape = np.mean(np.abs((inv_y.flatten() - inv_yhat.flatten()) /inv_y.flatten())) * 100\n",
        "  print(f'Mean absolute percentage error is: {mape}')\n",
        "  mae_error = mae(inv_y.flatten(),inv_yhat.flatten())\n",
        "  print(f'Mean Absolute Error is: {mae_error} ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7G9FeqQLzYu"
      },
      "source": [
        "X = data_response_monthly.copy()\n",
        "Multivariate_LSTM_timesteps(X,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYd0IcSPoUOe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}